{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWniMA742ZiyqMKDBWMoTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kinrz/RAFT-Motion-Blur/blob/main/RAFT_Motion_Blur_GUI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "idfUMo55Co4r"
      },
      "outputs": [],
      "source": [
        "\n",
        "# @title RAFT Motion Blur (GUI)\n",
        "# @markdown Jalankan sel ini dan tunggu sampai link gradio dibuat.\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from google.colab import output\n",
        "\n",
        "def install_packages():\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"gradio\"], check=True)\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"-qq\", \"ffmpeg\"], check=True)\n",
        "\n",
        "try:\n",
        "    import gradio as gr\n",
        "except ImportError:\n",
        "    install_packages()\n",
        "    import gradio as gr\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models.optical_flow import raft_large, Raft_Large_Weights\n",
        "import cv2\n",
        "import numpy as np\n",
        "import shutil\n",
        "from glob import glob\n",
        "import gc\n",
        "\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_model():\n",
        "    weights = Raft_Large_Weights.DEFAULT\n",
        "    transforms = weights.transforms()\n",
        "    model = raft_large(weights=weights, progress=False).to(device)\n",
        "    model.eval()\n",
        "    return model, transforms\n",
        "\n",
        "model, transforms = load_model()\n",
        "\n",
        "def get_grid(H, W):\n",
        "    y = torch.linspace(-1, 1, H, device=device)\n",
        "    x = torch.linspace(-1, 1, W, device=device)\n",
        "    gy, gx = torch.meshgrid(y, x, indexing='ij')\n",
        "    return torch.stack((gx, gy), dim=2).unsqueeze(0)\n",
        "\n",
        "def calculate_consistency(flow_fwd, flow_bwd):\n",
        "    N, C, H, W = flow_fwd.shape\n",
        "    base_grid = get_grid(H, W)\n",
        "    flow_fwd_norm = flow_fwd.permute(0, 2, 3, 1).clone()\n",
        "    flow_fwd_norm[..., 0] /= (W / 2.0)\n",
        "    flow_fwd_norm[..., 1] /= (H / 2.0)\n",
        "    grid = base_grid + flow_fwd_norm\n",
        "    warped_bwd = F.grid_sample(flow_bwd, grid, mode='bilinear', padding_mode='reflection', align_corners=True)\n",
        "    diff = flow_fwd + warped_bwd\n",
        "    magnitude = torch.norm(diff, dim=1, keepdim=True)\n",
        "    return torch.exp(-0.5 * magnitude)\n",
        "\n",
        "def apply_vector_blur_complex(img_tensor, flow_tensor, samples, strength, expansion, mask=None):\n",
        "    N, C, H, W = img_tensor.shape\n",
        "    base_grid = get_grid(H, W)\n",
        "    accumulated_img = torch.zeros_like(img_tensor)\n",
        "\n",
        "    processed_flow = flow_tensor.clone()\n",
        "    if expansion > 0:\n",
        "        k_size = int(expansion) * 2 + 1\n",
        "        processed_flow = TF.gaussian_blur(processed_flow, kernel_size=k_size, sigma=expansion)\n",
        "\n",
        "    flow_norm = processed_flow.permute(0, 2, 3, 1).clone()\n",
        "    flow_norm[..., 0] /= (W / 2.0)\n",
        "    flow_norm[..., 1] /= (H / 2.0)\n",
        "\n",
        "    for i in range(samples):\n",
        "        t = (i / (samples - 1)) - 0.5\n",
        "        offset = flow_norm * (t * strength)\n",
        "        sampling_grid = base_grid + offset\n",
        "        warped = F.grid_sample(img_tensor, sampling_grid, mode='bilinear', padding_mode='reflection', align_corners=True)\n",
        "        accumulated_img += warped\n",
        "\n",
        "    blurred = accumulated_img / samples\n",
        "\n",
        "    if mask is not None:\n",
        "        return (blurred * mask) + (img_tensor * (1 - mask))\n",
        "    else:\n",
        "        return blurred\n",
        "\n",
        "def pad_to_8(tensor):\n",
        "    h, w = tensor.shape[-2:]\n",
        "    new_h = ((h + 7) // 8) * 8\n",
        "    new_w = ((w + 7) // 8) * 8\n",
        "    ph = new_h - h\n",
        "    pw = new_w - w\n",
        "    if ph > 0 or pw > 0:\n",
        "        return F.pad(tensor, (0, pw, 0, ph)), h, w\n",
        "    return tensor, h, w\n",
        "\n",
        "def process_raft(input_video, blur_direction, tail_expansion, blur_strength, num_samples, flow_iterations, vram_safe_mode, safe_resolution, consistency_check, video_quality, progress=gr.Progress()):\n",
        "    if input_video is None:\n",
        "        return None\n",
        "\n",
        "    temp_dir = \"temp_frames_gradio\"\n",
        "    blur_dir = \"blur_frames_gradio\"\n",
        "    output_video = \"result.mp4\"\n",
        "\n",
        "    if os.path.exists(temp_dir): shutil.rmtree(temp_dir)\n",
        "    if os.path.exists(blur_dir): shutil.rmtree(blur_dir)\n",
        "    os.makedirs(temp_dir)\n",
        "    os.makedirs(blur_dir)\n",
        "\n",
        "    progress(0.05, desc=\"Extracting Frames...\")\n",
        "    subprocess.run(f\"ffmpeg -i {input_video} -pix_fmt rgb24 {temp_dir}/%08d.png -y -hide_banner -loglevel error\", shell=True)\n",
        "    frame_files = sorted(glob(f\"{temp_dir}/*.png\"))\n",
        "    total_frames = len(frame_files)\n",
        "\n",
        "    limit_res = safe_resolution if vram_safe_mode else 0\n",
        "\n",
        "    for i, curr_path in enumerate(frame_files):\n",
        "        progress((i / total_frames), desc=f\"Rendering Frame {i}/{total_frames}\")\n",
        "\n",
        "        if blur_direction == \"Forward\":\n",
        "            if i == total_frames - 1:\n",
        "                shutil.copy(curr_path, f\"{blur_dir}/{i:08d}.png\")\n",
        "                continue\n",
        "            target_path = frame_files[i+1]\n",
        "        else:\n",
        "            if i == 0:\n",
        "                shutil.copy(curr_path, f\"{blur_dir}/{i:08d}.png\")\n",
        "                continue\n",
        "            target_path = frame_files[i-1]\n",
        "\n",
        "        img1_orig = cv2.cvtColor(cv2.imread(curr_path), cv2.COLOR_BGR2RGB)\n",
        "        img2_orig = cv2.cvtColor(cv2.imread(target_path), cv2.COLOR_BGR2RGB)\n",
        "        orig_h, orig_w = img1_orig.shape[:2]\n",
        "\n",
        "        calc_h, calc_w = orig_h, orig_w\n",
        "        scale_factor = 1.0\n",
        "\n",
        "        if limit_res > 0 and (orig_w > limit_res or orig_h > limit_res):\n",
        "            if orig_w >= orig_h: scale_factor = limit_res / orig_w\n",
        "            else: scale_factor = limit_res / orig_h\n",
        "            calc_w, calc_h = int(orig_w * scale_factor), int(orig_h * scale_factor)\n",
        "            img1_small = cv2.resize(img1_orig, (calc_w, calc_h), interpolation=cv2.INTER_LINEAR)\n",
        "            img2_small = cv2.resize(img2_orig, (calc_w, calc_h), interpolation=cv2.INTER_LINEAR)\n",
        "        else:\n",
        "            img1_small, img2_small = img1_orig, img2_orig\n",
        "\n",
        "        img1_t_full = TF.to_tensor(img1_orig).unsqueeze(0).to(device)\n",
        "        img1_t_calc = TF.to_tensor(img1_small).unsqueeze(0).to(device)\n",
        "        img2_t_calc = TF.to_tensor(img2_small).unsqueeze(0).to(device)\n",
        "\n",
        "        img1_batch, img2_batch = transforms(img1_t_calc, img2_t_calc)\n",
        "        img1_pad, pad_h, pad_w = pad_to_8(img1_batch)\n",
        "        img2_pad, _, _ = pad_to_8(img2_batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            flow_main = model(img1_pad, img2_pad, num_flow_updates=flow_iterations)[-1]\n",
        "\n",
        "            consistency_mask = None\n",
        "            if consistency_check:\n",
        "                flow_rev = model(img2_pad, img1_pad, num_flow_updates=flow_iterations)[-1]\n",
        "                mask_pad = calculate_consistency(flow_main, flow_rev)\n",
        "                consistency_mask = mask_pad[:, :, :pad_h, :pad_w]\n",
        "\n",
        "            del img1_batch, img2_batch, img1_pad, img2_pad\n",
        "            if consistency_check: del flow_rev\n",
        "\n",
        "        flow_small = flow_main[:, :, :pad_h, :pad_w]\n",
        "\n",
        "        if scale_factor != 1.0:\n",
        "            flow_final = F.interpolate(flow_small, size=(orig_h, orig_w), mode='bilinear', align_corners=False)\n",
        "            flow_final *= (1.0 / scale_factor)\n",
        "            if consistency_mask is not None:\n",
        "                consistency_mask = F.interpolate(consistency_mask, size=(orig_h, orig_w), mode='bilinear', align_corners=False)\n",
        "        else:\n",
        "            flow_final = flow_small\n",
        "\n",
        "        blurred_tensor = apply_vector_blur_complex(img1_t_full, flow_final, num_samples, blur_strength, tail_expansion, consistency_mask)\n",
        "\n",
        "        res_img = (blurred_tensor.squeeze(0).permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "        res_img = cv2.cvtColor(res_img, cv2.COLOR_RGB2BGR)\n",
        "        cv2.imwrite(f\"{blur_dir}/{i:08d}.png\", res_img)\n",
        "\n",
        "        del flow_main, flow_small, flow_final, blurred_tensor, img1_t_full, img1_t_calc, img2_t_calc\n",
        "        if consistency_mask is not None: del consistency_mask\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    progress(0.98, desc=\"Encoding Video...\")\n",
        "    fps_cmd = f\"ffmpeg -i {input_video} 2>&1 | sed -n 's/.*, \\\\(.*\\\\) fp.*/\\\\1/p'\"\n",
        "    try:\n",
        "        orig_fps = subprocess.check_output(fps_cmd, shell=True).decode(\"utf-8\").strip()\n",
        "        if not orig_fps: orig_fps = 30\n",
        "    except: orig_fps = 30\n",
        "\n",
        "    subprocess.run(f\"ffmpeg -r {orig_fps} -i {blur_dir}/%08d.png -i {input_video} -map 0:v -map 1:a -c:a copy -c:v h264_nvenc -profile:v high -level 4.1 -preset p6 -tune hq -rc constqp -qp {video_quality} -pix_fmt yuv420p {output_video} -y -hide_banner -loglevel error\", shell=True)\n",
        "\n",
        "    return output_video\n",
        "\n",
        "rft_css = \"\"\"\n",
        "footer {visibility: hidden}\n",
        ".gradio-container {background-color: #1a1a1a !important;}\n",
        ".contain {background-color: #1a1a1a !important;}\n",
        "div[data-testid=\"block-label\"] {color: #cfcfcf !important; font-weight: bold;}\n",
        "label span {color: #cfcfcf !important;}\n",
        "\n",
        "button.primary {\n",
        "    background: linear-gradient(90deg, #7c3aed, #a855f7) !important;\n",
        "    color: white !important;\n",
        "    border: none !important;\n",
        "    box-shadow: 0 0 15px rgba(139, 92, 246, 0.6) !important;\n",
        "    transition: all 0.3s ease;\n",
        "}\n",
        "button.primary:hover {\n",
        "    box-shadow: 0 0 25px rgba(139, 92, 246, 0.9) !important;\n",
        "    transform: scale(1.02);\n",
        "}\n",
        "\n",
        "fieldset label {\n",
        "    background-color: transparent !important;\n",
        "    background-image: none !important;\n",
        "    border: 2px solid #8b5cf6 !important;\n",
        "    border-radius: 8px !important;\n",
        "    transition: all 0.3s ease;\n",
        "    margin-right: 5px;\n",
        "}\n",
        "\n",
        "fieldset label.selected {\n",
        "    background-color: rgba(139, 92, 246, 0.1) !important;\n",
        "    border-color: #8b5cf6 !important;\n",
        "    box-shadow: 0 0 10px #8b5cf6, inset 0 0 5px rgba(139, 92, 246, 0.3) !important;\n",
        "    color: #ffffff !important;\n",
        "    font-weight: normal !important;\n",
        "}\n",
        "\n",
        "h1 {\n",
        "    color: #a855f7;\n",
        "    text-shadow: 0 0 10px rgba(139, 92, 246, 0.5);\n",
        "    font-family: 'Courier New', monospace;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "rft_theme = gr.themes.Base(\n",
        "    primary_hue=gr.themes.colors.purple,\n",
        "    neutral_hue=gr.themes.colors.slate,\n",
        "    font=[gr.themes.GoogleFont('Inter'), 'ui-sans-serif', 'system-ui', 'sans-serif'],\n",
        ").set(\n",
        "    body_background_fill='#161618',\n",
        "    body_text_color='#eaeaea',\n",
        "    block_background_fill='#25252b',\n",
        "    block_border_width='0px',\n",
        "    block_label_background_fill='#25252b',\n",
        "    block_title_text_color='#ffffff',\n",
        "    input_background_fill='#1a1a1f',\n",
        "    button_primary_background_fill='#7c3aed',\n",
        "    button_primary_background_fill_hover='#6d28d9',\n",
        "    slider_color='#8b5cf6'\n",
        ")\n",
        "\n",
        "with gr.Blocks(theme=rft_theme, css=rft_css, title=\"RAFT Motion Blur\") as demo:\n",
        "    with gr.Row():\n",
        "        gr.Markdown(\"# RAFT Motion Blur\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            input_video = gr.Video(label=\"Input Video\", sources=[\"upload\"])\n",
        "            with gr.Accordion(\"Settings\", open=True):\n",
        "                blur_direction = gr.Radio([\"Forward\", \"Backward\"], value=\"Backward\", label=\"Blur Direction\", info=\"Forward: Hitung dari frame depan. Backward: Hitung dari frame belakang.\")\n",
        "                tail_expansion = gr.Slider(0, 100, value=40, step=5, label=\"Tail Expansion\", info=\"Feathering tepi blur (0= Tajam, 30+= Halus).\")\n",
        "                blur_strength = gr.Slider(0.1, 3.0, value=1.0, step=0.05, label=\"Blur Strength\", info=\"Panjang blur.\")\n",
        "                num_samples = gr.Slider(16, 128, value=128, step=8, label=\"Blur Samples\", info=\"Kualitas/Kehalusan blur.\")\n",
        "                flow_iterations = gr.Slider(10, 40, value=16, step=2, label=\"Flow Iterations\", info=\"Akurasi frame tracking (Higher = Slower).\")\n",
        "\n",
        "            with gr.Accordion(\"Advanced\", open=False):\n",
        "                with gr.Row():\n",
        "                    vram_safe_mode = gr.Checkbox(value=False, label=\"Safe Mode\", info=\"Jika video resolusi tinggi sering gagal render.\")\n",
        "                    consistency_check = gr.Checkbox(value=False, label=\"Consistency Check\", info=\"Experimental.\")\n",
        "                safe_resolution = gr.Slider(960, 1280, value=1280, step=64, label=\"Safe Mode Limit\", info=\"Resolusi baca engine jika Safe Mode ON.\")\n",
        "                video_quality = gr.Slider(15, 30, value=19, step=1, label=\"Video Quality (CRF)\", info=\"Semakin kecil semakin jernih.\")\n",
        "\n",
        "            btn_run = gr.Button(\"RENDER\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_video = gr.Video(label=\"Result Preview\", interactive=False)\n",
        "            gr.Markdown(\"> **Tip:** jika gagal render. Nyalakan *Safe Mode*.\")\n",
        "\n",
        "    btn_run.click(\n",
        "        fn=process_raft,\n",
        "        inputs=[input_video, blur_direction, tail_expansion, blur_strength, num_samples, flow_iterations, vram_safe_mode, safe_resolution, consistency_check, video_quality],\n",
        "        outputs=output_video\n",
        "    )\n",
        "\n",
        "demo.queue().launch(share=True, inline=False, debug=False, show_api=False)"
      ]
    }
  ]
}